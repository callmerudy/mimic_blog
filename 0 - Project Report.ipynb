{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting 30 day readmissions using the MIMIC-III dataset\n",
    "\n",
    "#### [Project Report](#report)\n",
    "\n",
    "#### [Deep Dive]()\n",
    "[Getting Started with Our Model](#intro)   \n",
    "[Preprocessing the Data](#preprocessing)  \n",
    "[Extracting Basic Features](#features)  \n",
    "[Creating Features from Diagnosis Codes](#dx_features)    \n",
    "[Extracting Features into a Pandas Dataframe](#pandas)  \n",
    "[Understanding How to Evaluate our Classifier](#pandas)  \n",
    "[Fitting the Model and Plotting the ROC Curve](#pandas)  \n",
    "[Results, Analysis, and Future Work](#pandas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='report'></a>\n",
    "# Predicting 30 Day Readmission from the MIMIC-III Dataset\n",
    "\n",
    "#### Problem \n",
    "Hospital readmissions are both common and costly within the US healthcare system, and recent healthcare policy has paid particular attention to creating incentives to reduce 30 day readmission rates within hospitals <sup>[1](#report_bib)</sup>. In particular, provisions within the American Care Act (ACA) have created financial incentives for hospitals to minimize readmissions shortly after discharge.  \n",
    "\n",
    "<br></br>\n",
    "This project outlines an effort to build a model that predicts emergency department readmission within 30 days of patient discharge. In order to build and train this model, we used data from the open source MIMIC-III dataset <sup>[2](#report_bib)</sup>, which contains anonymized demographic and medical data for more than 40,000 patients followed over ten years.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "#### Methods\n",
    "\n",
    "Data was extracted from the MIMIC-III dataset, and transformed in order to match the schema provided by the OMOP Common Data Model <sup>[3](#report_bib)</sup> for observational data. Individual patient index records were selected and used to create records of readmission for each patient. Relevant data features were selected or constructed from demographic, admission, and diagnostic records. Features were built around previous admission types, prior readmission rates, demographic data (such as marital status, ethnicity, age, and gender), and type of insurance at admission. Additionally, we created features around the concept of patient comorbidity, as defined by the Charlson Comorbidity Index <sup>[4](#report_bib)</sup>. Diagnostic data features were included through grouping ICD9CM codes based on an implementation of AHRQ's CCS ICD9 code clustering mapping <sup>[5](#report_bib)</sup>.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "We used a k-fold cross validation scheme and bootstrapping for data resampling. We then applied open-source implementations of ensemble classifiers (from the popular Python library scikit-learn), particularly an implementation of a Gradient Tree Boosting <sup>[6](#report_bib)</sup> classifier. The AUC (area under the curve) was calculated for each k-fold from the data, and the mean AUC was calculated to build a general ROC curve for the classifier.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "#### Results\n",
    "\n",
    "<img src='http://i.imgur.com/3hBmCQu.png' width=\"580\" height=\"580\" align='right' />\n",
    "\n",
    "We ran our model against various out-of-the-box classifiers, and found consistant best results with sk-learn GradientBoostingClassifier. \n",
    "\n",
    "<br></br>\n",
    "\n",
    "Using 63 total features, including demographic, insurance, admission data, and morbidity and CCS features constructed from ICD9 codes, **our model produced a mean AUC of 0.813.** \n",
    "\n",
    "<br></br>\n",
    "\n",
    "\n",
    "By evaluating the same dataset but eliminating feature sets, such as CCS and Charlson calculations, we were able to gain insight into the additional predictive power that using condition and diagnostic data provided for our dataset. AUCs for subsets of the total feature set: \n",
    "- Without insurance data features: 0.812 AUC (limited effect) \n",
    "- Without CCS condition data features: 0.705 AUC (significant effect) \n",
    "- Without Charlson data features: TBD\n",
    "- Just demographic information: TBD\n",
    "\n",
    "<br></br>\n",
    "\n",
    "#### Insights and future work\n",
    "\n",
    "We're pleased with the performance of our model on this data, and we believe this project shows how feature engineering can effectively support a predictive model with healthcare data. Our approach doesn't utilize some of the more cutting edge machine learning techniques that could be applied, but seems to rank respectably against many recent approaches <sup>[7](#report_bib)</sup>. \n",
    "\n",
    "<br></br>\n",
    "Here's some more basic additional avenues we may explore to improve the model: \n",
    "- We're currently using least granular level of [CCS categorization](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/CCSUsersGuide.pdf) [pdf] to create a limited set of condition features. Re-running the feature extraction with a more comprehensive level of categorization could result in improved accuracy. \n",
    "- The MIMIC-III dataset has some data on admission and discharge free form notes. Basic NLP techniques could be applied to these text records to try to create additional features. \n",
    "- Patient prescription records remain unexplored, as do lab test result data. \n",
    "- Continued hyperparameter adjustments and exploriting alternative machine learning algorithms and setups could yield additional value. \n",
    "\n",
    "<br></br>\n",
    "\n",
    "\n",
    "#### References\n",
    "\n",
    "1. Joynt, K.E. and Jha, A.K. [Thirty-day readmissions – truth and consequences](http://www.nejm.org/doi/full/10.1056/NEJMp1201598#t=article). The New England Journal of Medicine. 2012; 366: 1366–1369\n",
    "- Johnson\tAEW, Pollard TJ, Shen L, Lehman\tL, Feng\tM, Ghassemi\tM, Moody B,\tSzolovits P, Celi LA, and Mark RG. [MIMIC-III, a freely\taccessible\tcritical care database](https://www.nature.com/articles/sdata201635/metrics). Scientific Data (2016). 10.1038/sdata.2016.35.\n",
    "- Observational Medical Outcomes Partnership. OMOP Common Data Model. Observational Medical Outcomes Partnership; 2013. Available from: http://omop.org/CDM.\n",
    "- Charlson M, Szatrowski T, Peterson J, et al. [Validation of a combined comorbidity index](https://www.ncbi.nlm.nih.gov/pubmed/7722560). J Clin Epidemiol. 1994;47:1245e51.\n",
    "- Healthcare Cost and Utilization Project (HCUP). Clinical Classifications Software. Available from: https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp. \n",
    "- [Scikit-learn: Machine Learning in Python](http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. \n",
    "- Tong L, Erdmann C, Daldalian M, et al. [Comparison of predictive modeling approaches for 30-day all-cause non-elective readmission risk](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4769572/#CR6). BMC Med Res Methodol 2016;16:1–8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
