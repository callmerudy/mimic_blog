{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "html {\n",
       "  font-size: 62.5% !important; }\n",
       "body {\n",
       "  font-size: 1.5em !important; /* currently ems cause chrome bug misinterpreting rems on body element */\n",
       "  line-height: 1.6 !important;\n",
       "  font-weight: 400 !important;\n",
       "  font-family: \"Raleway\", \"HelveticaNeue\", \"Helvetica Neue\", Helvetica, Arial, sans-serif !important;\n",
       "  color: #222 !important; }\n",
       "\n",
       "div{ border-radius: 0px !important;  }\n",
       "div.CodeMirror-sizer{ background: rgb(244, 244, 248) !important; }\n",
       "div.input_area{ background: rgb(244, 244, 248) !important; }\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  color: #333 !important;\n",
       "  margin-top: 0 !important;\n",
       "  margin-bottom: 2rem !important;\n",
       "  font-weight: 300 !important; }\n",
       "h1 { font-size: 4.0rem !important; line-height: 1.2 !important;  letter-spacing: -.1rem !important;}\n",
       "h2 { font-size: 3.6rem !important; line-height: 1.25 !important; letter-spacing: -.1rem !important; }\n",
       "h3 { font-size: 3.0rem !important; line-height: 1.3 !important;  letter-spacing: -.1rem !important; }\n",
       "h4 { font-size: 2.4rem !important; line-height: 1.35 !important; letter-spacing: -.08rem !important; }\n",
       "h5 { font-size: 1.8rem !important; line-height: 1.5 !important;  letter-spacing: -.05rem !important; }\n",
       "h6 { font-size: 1.5rem !important; line-height: 1.6 !important;  letter-spacing: 0 !important; }\n",
       "\n",
       "p {\n",
       "  margin-top: 0 !important; }\n",
       "  \n",
       "a {\n",
       "  color: #1EAEDB !important; }\n",
       "a:hover {\n",
       "  color: #0FA0CE !important; }\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "import urllib2\n",
    "HTML(urllib2.urlopen('https://gist.githubusercontent.com/mattlewissf/83989910849fdb4a04a72d431e84053f/raw/cefa015a9065665faccd0219774c7087be7d21a8/skeleton.css').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MIMIC Deep Dive - Results, Analysis, and Future Work\n",
    "**[Intro](#intro)**   \n",
    "**[30 Day Readmission](#30_day_readmission)**  \n",
    "**[The MIMIC Dataset](#mimic_dataset)**  \n",
    "**[Setting up the database](#setting_up_db)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen before, GBC gives us the highest AUC: \n",
    "\n",
    "- **GradientBoostingClassifier - 0.8127**\n",
    "- AdaBoostClassifier - 0.8037\n",
    "- LogisticRegression -  0.7769\n",
    "- LogisticRegressionCV - 0.7718\n",
    "- DecisionTreeClassifier - 0.7452\n",
    "- RandomForestClassifier -  0.6706\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put in ROC curves (super small, maybe stiched together, for all of them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingClassifier\n",
    "\n",
    "Sk-learn's GradientBoostingClassifier (GBC) is (unsurprisingly) an example of a gradient boosted model (GBM). GBMs, especially the XGBoost algorith, are popular-used ensamble models for classification (and other) tasks. **Boosting** refers to a sequential turning **weak learners** into an ensemble strong learner classifier. In short: GBC starts with a rough prediction and then works to build a series of decision trees, with each tree in the series trying to correct the previous prediction error. \n",
    "\n",
    "<br></br>\n",
    "\n",
    "To quote a [higher power](\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/):  \n",
    "> The common ensemble techniques like random forests rely on simple averaging of models in the ensemble. The family of boosting methods is based on a different.... add[ing] new models to the ensemble sequentially. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far....In gradient boosting machines, or simply, GBMs, the learning procedure consecutively fits new models to provide a more accurate estimate of the response variable.\n",
    "\n",
    "\n",
    "Here's some additional resources on GBMs: \n",
    "- Amazing interactive graphics: http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html\n",
    "- A paper with a gentle introduction; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our highest AUC comes from using GBC - but can we get more out of the classifier itself? Most of our work so far has been extracting feature - or ** model parameters** - from the data itself to give ourselves the best chance of good predicitve abilities. Parameters can be learned from the training process, and that's what we are trying to do when we fit the model to our data. \n",
    "\n",
    "**Hyperparameters** are properties of the model itself that can't be learned directly from the data, and so need to be predefined. These can be things like the set number of leaves in a tree model. For GBC, sk-learn gives really easy access to the tunable hyperparameters. Here's the default settings: \n",
    "\n",
    "``` \n",
    "GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_split=1e-07, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto'\n",
    "```\n",
    "\n",
    "After reading a decent amount about trying to tune GBC classifers with sk-learn, I decided to experiment a bit with some of the more commonly 'tuned' hyperparameters to see if I could squeeze some additional AUC from our GBC. I foucused on varying max_depth, the number of estimateors, and the learning rate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GBC_attempts = {\n",
    "                'standard': GradientBoostingClassifier(),\n",
    "                'depth_5': GradientBoostingClassifier(max_depth=5), \n",
    "                'depth_2': GradientBoostingClassifier(max_depth=2), \n",
    "                'depth_4': GradientBoostingClassifier(max_depth=1), \n",
    "                'max_features_auto': GradientBoostingClassifier(max_features=\"auto\"),\n",
    "                'n_est_200': GradientBoostingClassifier(n_estimators=200), \n",
    "                'n_est_40': GradientBoostingClassifier(n_estimators=40),\n",
    "                'n_est_40_l_rate_05': GradientBoostingClassifier(n_estimators=40, learning_rate=.05) }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what happened when I tried to turn all of the knobs? Here's the results, pitted against the out-of-the-box setup: \n",
    "\n",
    "Some stuff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
